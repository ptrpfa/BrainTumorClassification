{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Brain Tumour Classifier, based on Google's Xception Model\n",
    "References:\n",
    "- https://keras.io/api/applications/xception/\n",
    "- https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568\n",
    "- https://www.kaggle.com/code/qaidjamali/brain-tumor-classification-98-acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required functions and libraries\n",
    "from utils import *\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "img_batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_size = (img_height, img_width)\n",
    "img_shape = img_size + (3,)\n",
    "\n",
    "# Get training and validation datasets\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  DATASET_FOLDER,\n",
    "  validation_split=0.2,\n",
    "  subset=\"both\",\n",
    "  shuffle=True,\n",
    "  seed=RANDOM_STATE,\n",
    "  image_size=img_size,\n",
    "  batch_size=img_batch_size)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_ds.class_names\n",
    "# class_names = val_ds.class_names\n",
    "\n",
    "# Enable buffered prefetching to overlap data preprocessing and model execution during training, to speed up access to dataset\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview datasets\n",
    "print(\"\\nTraining Dataset Preview\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Dataset Preview\")\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in val_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the base model\n",
    "Create a base model, by attaching a new classification head on top of the EfficientNetB0 model for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\", input_shape=img_shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dropout(rate=0.25),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise model performance throughout epochs\n",
    "# Accuracy\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "# Loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "# Get epoch of optimal parameters\n",
    "print(\"Training Results:\")\n",
    "print(\"~Accuracy~\")\n",
    "print(\"Max (best) accuracy:\", max(history.history['accuracy']), \"at epoch\", history.history['accuracy'].index(max(history.history['accuracy'])) + 1)\n",
    "print(\"Min (worst) accuracy:\", min(history.history['accuracy']), \"at epoch\", history.history['accuracy'].index(min(history.history['accuracy'])) + 1)\n",
    "print(\"Median accuracy:\", np.median(history.history['accuracy']))\n",
    "print(\"Mean accuracy:\", np.mean(history.history['accuracy']))\n",
    "\n",
    "print(\"\\n~Loss~\")\n",
    "print(\"Min (best) loss:\", min(history.history['loss']), \"at epoch\", history.history['loss'].index(min(history.history['loss'])) + 1)\n",
    "print(\"Max (worst) loss:\", max(history.history['loss']), \"at epoch\", history.history['loss'].index(max(history.history['loss'])) + 1)\n",
    "print(\"Median loss:\", np.median(history.history['loss']))\n",
    "print(\"Mean loss:\", np.mean(history.history['loss']))\n",
    "\n",
    "print(\"\\n~Final Training Performance~\")\n",
    "print(\"Accuracy: {1}, Loss: {0}\".format(*model.evaluate(train_ds, verbose=0)))\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"~Accuracy~\")\n",
    "print(\"Max (best) accuracy:\", max(history.history['val_accuracy']), \"at epoch\", history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1)\n",
    "print(\"Min (worst) accuracy:\", min(history.history['val_accuracy']), \"at epoch\", history.history['val_accuracy'].index(min(history.history['val_accuracy'])) + 1)\n",
    "print(\"Median accuracy:\", np.median(history.history['val_accuracy']))\n",
    "print(\"Mean accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "\n",
    "print(\"\\n~Loss~\")\n",
    "print(\"Min (best) loss:\", min(history.history['val_loss']), \"at epoch\", history.history['val_loss'].index(min(history.history['val_loss'])) + 1)\n",
    "print(\"Max (worst) loss:\", max(history.history['val_loss']), \"at epoch\", history.history['val_loss'].index(max(history.history['val_loss'])) + 1)\n",
    "print(\"Median loss:\", np.median(history.history['val_loss']))\n",
    "print(\"Mean loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print(\"\\n~Final Validation Performance~\")\n",
    "print(\"Accuracy: {1}, Loss: {0}\".format(*model.evaluate(val_ds, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"~Training Metrics~\")\n",
    "get_model_metrics(train_ds, model)\n",
    "\n",
    "print(\"\\n~Validation Metrics~\")\n",
    "get_model_metrics(val_ds, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation to validate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" K-Fold Cross-Validation \"\"\"\n",
    "# Load dataset without validation split\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_FOLDER,\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_STATE,\n",
    "    image_size=img_size,\n",
    "    batch_size=img_batch_size\n",
    ")\n",
    "\n",
    "# Convert dataset to numpy arrays for k-fold splitting\n",
    "images = []\n",
    "labels = []\n",
    "for image, label in dataset:\n",
    "    images.append(image.numpy())\n",
    "    labels.append(label.numpy())\n",
    "images = np.concatenate(images)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Initialise lists to store results\n",
    "list_kf_results = []\n",
    "list_skf_results = []\n",
    "\n",
    "print(\"K-Fold Cross-Validation\")\n",
    "fold_no = 0\n",
    "for train_index, val_index in kf.split(images):\n",
    "    # Split data\n",
    "    train_images, val_images = images[train_index], images[val_index]\n",
    "    train_labels, val_labels = labels[train_index], labels[val_index]\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "\n",
    "    # Prefetch datasets\n",
    "    train_ds = train_ds.batch(img_batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(img_batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(train_ds, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
    "    \n",
    "    # Save results\n",
    "    list_kf_results.append({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"history\": history})\n",
    "    \n",
    "    print(\"Fold Index\", fold_no)\n",
    "    print(\"~Training Dataset~\\nAccuracy: {}, Loss: {}\".format(train_accuracy, train_loss))\n",
    "    print(\"~Validation Dataset~\\nAccuracy: {}, Loss: {}\".format(val_accuracy, val_loss), end=\"\\n\\n\")\n",
    "    \n",
    "    # Increment fold number\n",
    "    fold_no += 1\n",
    "    \n",
    "print(\"\\nStratified K-Fold Cross-Validation\")\n",
    "fold_no = 0\n",
    "for train_index, val_index in skf.split(images, labels):\n",
    "    # Split data\n",
    "    train_images, val_images = images[train_index], images[val_index]\n",
    "    train_labels, val_labels = labels[train_index], labels[val_index]\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "\n",
    "    # Prefetch datasets\n",
    "    train_ds = train_ds.batch(img_batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(img_batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Input layer\n",
    "    inputs = keras.Input(shape=img_shape)\n",
    "\n",
    "    # Preprocessing layers (data augmetation)\n",
    "    x = tf.keras.layers.RandomFlip('horizontal')(inputs)\n",
    "    x = tf.keras.layers.RandomRotation(0.2)(x)\n",
    "    x = tf.keras.layers.RandomZoom(0.2)(x)\n",
    "\n",
    "    # Base model layer, ensure that the model is running in inference mode\n",
    "    base_model = tf.keras.applications.MobileNetV3Small(input_shape=img_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "    x = base_model(x, training=False) \n",
    "\n",
    "    # Processing layers\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer (use softmax for multi-label classification task)\n",
    "    outputs = keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    # Train the model\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(train_ds, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
    "    \n",
    "    # Save results\n",
    "    list_skf_results.append({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"history\": history})\n",
    "    \n",
    "    print(\"Fold Index\", fold_no)\n",
    "    print(\"~Training Dataset~\\nAccuracy: {}, Loss: {}\".format(train_accuracy, train_loss))\n",
    "    print(\"~Validation Dataset~\\nAccuracy: {}, Loss: {}\".format(val_accuracy, val_loss), end=\"\\n\\n\")\n",
    "    \n",
    "    # Increment fold number\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_kf_results\n",
    "\n",
    "print(\"Training Results:\")\n",
    "print(\"~Accuracy~\")\n",
    "print(\"Max (best) accuracy:\", max(history.history['accuracy']), \"at epoch\", history.history['accuracy'].index(max(history.history['accuracy'])) + 1)\n",
    "print(\"Min (worst) accuracy:\", min(history.history['accuracy']), \"at epoch\", history.history['accuracy'].index(min(history.history['accuracy'])) + 1)\n",
    "print(\"Median accuracy:\", np.median(history.history['accuracy']))\n",
    "print(\"Mean accuracy:\", np.mean(history.history['accuracy']))\n",
    "\n",
    "print(\"\\n~Loss~\")\n",
    "print(\"Min (best) loss:\", min(history.history['loss']), \"at epoch\", history.history['loss'].index(min(history.history['loss'])) + 1)\n",
    "print(\"Max (worst) loss:\", max(history.history['loss']), \"at epoch\", history.history['loss'].index(max(history.history['loss'])) + 1)\n",
    "print(\"Median loss:\", np.median(history.history['loss']))\n",
    "print(\"Mean loss:\", np.mean(history.history['loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "def calculate_statistics(data):\n",
    "    return {\n",
    "        'max': np.max(data),\n",
    "        'min': np.min(data),\n",
    "        'median': np.median(data),\n",
    "        'mean': np.mean(data)\n",
    "    }\n",
    "\n",
    "# Extract values\n",
    "train_accuracies = [result['train_accuracy'] for result in list_kf_results]\n",
    "train_losses = [result['train_loss'] for result in list_kf_results]\n",
    "val_accuracies = [result['val_accuracy'] for result in list_kf_results]\n",
    "val_losses = [result['val_loss'] for result in list_kf_results]\n",
    "\n",
    "# Print results\n",
    "print(json.dumps({\n",
    "    'Training Accuracy': calculate_statistics(train_accuracies),\n",
    "    'Training Loss': calculate_statistics(train_losses),\n",
    "    'Validation Accuracy': calculate_statistics(val_accuracies),\n",
    "    'Validation Loss': calculate_statistics(val_losses)\n",
    "}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "model.save(MODEL_FILE)\n",
    "\n",
    "# Load the trained model\n",
    "# model = keras.models.load_model(MODEL_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
